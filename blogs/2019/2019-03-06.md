# Docker EE 中的 Swarm Mode 网络

Docker Enterprise Edition 支持 swarm mode. In this section you'll work through examples of a few basic networking concepts, learn about Bridge and Overlay networking, and finally learning about the Swarm Routing Mesh.

> **难度**: 初级、中级

> **时间**:  45 分钟

> **任务目录**:
>
> * [Section #1 - 网络基础](#task1)
> * [Section #2 - Bridge Networking](#task2)
> * [Section #3 - Overlay Networking](#task3)

## 准备工作 - 安装几个工具

PWD is a minimal environment, so we will need to add a few tools to help with the workshop. On the **manager1** node run:

```bash
sudo apt-get update && apt-get install -y curl dnsutils iputils-ping
```

And run the same command on **worker1**:

```bash
sudo apt-get update && apt-get install -y curl dnsutils iputils-ping
```

## <a name="task1"></a>Section #1 - 网络基础

### <a name="list_networks"></a>Step 1: Docker 网络命令

Connect to the **manager1** node in your PWD session to explore the existing Docker networks.

 `docker network` 指令配置和管理容器网络的主要指令.在**manager1** 上运行`docker network` 命令.

```bash
$ docker network

Usage:    docker network COMMAND

Manage networks

Options:
      --help   Print usage

Commands:
  connect     Connect a container to a network
  create      Create a network
  disconnect  Disconnect a container from a network
  inspect     Display detailed information on one or more networks
  ls          List networks
  prune       Remove all unused networks
  rm          Remove one or more networks

Run 'docker network COMMAND --help' for more information on a command.
```

The command output shows how to use the command as well as all of the `docker network` sub-commands. As you can see from the output, the `docker network` command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks.

### <a name="list_networks"></a>Step 2: 列出网络

Run a `docker network ls` command on **manager1** to view existing container networks on the current Docker host.

```bash
$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
bf49ba724655        bridge              bridge              local
f88f42dbcd4c        docker_gwbridge     bridge              local
qcagnwr8f6xh        dtr-ol              overlay             swarm
f34066c16e6a        host                host                local
86zgov3fztu8        ingress             overlay             swarm
69826fd119d8        none                null                local
```

The output above shows the container networks that are created as part of a standard installation of Docker EE.

> These networks are cluster-wide in Docker EE. You can run the `docker network ls` command on any other node and you will see the same output.

New networks that you create will also show up in the output of the `docker network ls` command.

You can see that each network gets a unique `ID` and `NAME`. Each network is also associated with a single driver. Notice that the "bridge" network and the "host" network have the same name as their respective drivers.

### <a name="inspect"></a>Step 3: Inspect a network

The `docker network inspect` command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more.

Use `docker network inspect <network>` on **manager1** to view configuration details of the container networks on your Docker host. The command below shows the details of the network called `bridge`.

```bash
$ docker network inspect bridge
[
    {
        "Name": "bridge",
        "Id": "3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57",
        "Created": "2017-04-03T16:49:58.6536278Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.0/16",
                    "Gateway": "172.17.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Containers": {},
        "Options": {
            "com.docker.network.bridge.default_bridge": "true",
            "com.docker.network.bridge.enable_icc": "true",
            "com.docker.network.bridge.enable_ip_masquerade": "true",
            "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
            "com.docker.network.bridge.name": "docker0",
            "com.docker.network.driver.mtu": "1500"
        },
        "Labels": {}
    }
]
```

> **备注:** The syntax of the `docker network inspect` command is `docker network inspect <network>`, where `<network>` can be either network name or network ID. In the example above we are showing the configuration details for the network called "bridge". Do not confuse this with the "bridge" driver.

### <a name="list_drivers"></a>Step 4: 列出网络驱动插件

Docker has a plugin system to enable different network topologies using different network drivers.

You can see all the installed plugins (for volumes, networks and logging) from the `docker info` command.

Run `docker info` on **manager1** and locate the list of network plugins:

```bash
$ docker info
Containers: 31
 Running: 27
 Paused: 0
 Stopped: 4
Images: 24
Server Version: 17.06.2-ee-11
...
Plugins:
 Volume: local
 Network: bridge host ipvlan macvlan null overlay
 Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog
...
```

The output above shows the **bridge**, **host**, **ipvlan**, **macvlan**, **null**, and **overlay** drivers are installed. In this workshop you'll learn about the bridge and overlay networks.

## <a name="task2"></a>Section #2 - Bridge 网络

[Bridge networks](https://docs.docker.com/network/bridge/) are used for connecting Docker containers running on a single server. Containers connected to the same bridge network can reach each other, and they can also reach external resources that the server can access.

### <a name="connect-container"></a>Step 1: 基础

Every clean installation of Docker comes with a pre-built network called **bridge**. Verify this with the `docker network ls` command on **manager1**. filtering on the network driver:

```bash
$ docker network ls --filter driver=bridge
NETWORK ID          NAME                DRIVER              SCOPE
bf49ba724655        bridge              bridge              local
f88f42dbcd4c        docker_gwbridge     bridge              local
```

The output above shows two networks associated with the *bridge* driver:

* **bridge** is a default network created when you install Docker, even in a single-node installation

* **docker_gwbridge** is a default network created when you deploy UCP in Docker EE

The output above also shows that the bridge networks are scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the *bridge* driver - the *bridge* driver provides single-host networking.

> In a Docker EE cluster, these networks are replicated on all the nodes. Every node has its own **bridge** and **docker_gwbridge** network, but each is local to the node and there is no traffic between them.

All networks created with the *bridge* driver are based on a Linux bridge (a.k.a. a virtual switch).

### <a name="connect-container"></a>Step 2: 连接到容器

The **bridge** network is the default network for new containers. This means that unless you specify a different network, all new containers will be connected to the **bridge** network.

Create a new bridge network on **manager1** and call it `br`.

```bash
docker network create -d bridge br
```

> This custom network only exists on node **manager1**. If you list the networks on another node, you will not see the new **br** network.

Now create a container called `c1` and attach it to your new `br` network on **manager1**.

```bash
docker container run -itd --net br --name c1 alpine sh
```

This command will create a new container based on the `alpine:latest` image.

Running `docker network inspect br` will show the containers on that network.

```bash
$ docker network inspect br

[
    {
        "Name": "br",
        "Id": "056b27c1488e56bda4e11ca0937166f0512031cddd4f4b68ff5bb2a1d11b136a",
        "Created": "2018-05-23T10:53:43.594987763Z",
        "Scope": "local",
        "Driver": "bridge",
        ...
        "Containers": {
            "f849e0f45933da5f2324f132ed6224511fc6c8ea95532858ee6753a2a1054746": {
                "Name": "c1",
                "EndpointID": "f43ccc304a907615fd341b195d2bcce5761c0ab4b9f19ae52a24e56b25b29eb0",
                "MacAddress": "02:42:ac:14:00:02",
                "IPv4Address": "172.20.0.2/16",
                "IPv6Address": ""
            }
        },
        ...
    }
]
```


### <a name="ping_local"></a>Step 3: 测试网络连通性

The output to the previous `docker network inspect` command shows the IP address of the new container. In the previous example it is `172.20.0.2` but yours might be different.

> You can get the container's IP address for the **br** network by inspecting the container and formatting the response: `docker container inspect c1 --format "{{ .NetworkSettings.Networks.br.IPAddress }}"`

Ping the IP address of the container from the shell prompt of your Docker host by running `ping -c 3 <IPv4 Address>` on **node0**. Remember to use the IP of the container in **your** environment.

You can get the IP address of the **c1** container directly from the Docker engine by running `docker inspect --format "{{ .NetworkSettings.Networks.br.IPAddress }}" c1`.

```bash
$ ping -c 3 172.20.0.2
PING 172.20.0.2 (172.20.0.2) 56(84) bytes of data.
64 bytes from 172.20.0.2: icmp_seq=1 ttl=64 time=0.087 ms
64 bytes from 172.20.0.2: icmp_seq=2 ttl=64 time=0.067 ms
64 bytes from 172.20.0.2: icmp_seq=3 ttl=64 time=0.048 ms
...
```

The replies above show that the Docker host can ping the container over the **bridge** network. But, we can also verify the container can connect to the outside world too.

Enter in to the **c1** container that you created using the command `docker container exec`. We will pass the `sh` command to `container exec` which puts us in to an interactive shell inside the container.

Enter in to the container and inspect the interfaces of the container:

```bash
$ docker exec -it c1 sh

 # ip addr show | grep inet
    inet 127.0.0.1/8 scope host lo
    inet 172.20.0.2/16 scope global eth
```

The IP address of the container is accessible from the host. Now prove that containers can gain outside access by pinging `www.docker.com`.

```bash
 # ping -c 3 www.docker.com

PING www.docker.com (52.85.101.235): 56 data bytes
64 bytes from 52.85.101.235: seq=0 ttl=237 time=7.428 ms
64 bytes from 52.85.101.235: seq=1 ttl=237 time=7.451 ms
64 bytes from 52.85.101.235: seq=2 ttl=237 time=7.406 ms
```

退出容器.

```bash
 # exit
```

Now you will create a second container on this bridge so you can test connectivity between them.

```bash
docker container run -itd --net br --name c2 alpine sh
```

Containers **c1** and **c2** are connected to the same bridge network, so they are visible to each other and to the Docker host.

Use `docker container exec` to ping the IP address of the **c1** container from **c2**:

```bash
$ docker container exec -it c2 ping -c 2 172.20.0.2
PING 172.20.0.2 (172.20.0.2): 56 data bytes
64 bytes from 172.20.0.2: seq=0 ttl=64 time=0.072 ms
64 bytes from 172.20.0.2: seq=1 ttl=64 time=0.070 ms
...
```

Now ping container **c1** using it's name:

```bash
$ docker container exec -it c2 ping -c 2 c1
PING c1 (172.20.0.2): 56 data bytes
64 bytes from 172.20.0.2: seq=0 ttl=64 time=0.095 ms
64 bytes from 172.20.0.2: seq=1 ttl=64 time=0.108 ms
...
```

The Docker engine provides DNS resolution automatically for all container names and service names.

We've been running containers directly on the **manager1** node, using a private network on that node. Connect to **worker1** and verify that the container isn't accessible on that node:

```bash
$ ping -c 2 -W 2 172.20.0.2
PING 172.20.0.2 (172.20.0.2) 56(84) bytes of data.

--- 172.20.0.2 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 1003ms
```

> The container IP address is private to the host when you're using bridge networking.

Now switch back to **manager1** and remove the containers:

```bash
docker rm -f c1
docker rm -f c2
```

### <a name="nat"></a>Step 4: 为外部连结性配置 NAT

In this step we'll start a new **NGINX** container and map port 8000 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8000 will be passed on to port 80 inside the container.

> **NOTE:** If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80.

Start a new container based off the official NGINX image by running a container on **manager1**.

```bash
docker container run --name web1 -d -p 8000:80 nginx:alpine
```

Review the container status and port mappings by running `docker container ls` on **manager1**, filtering fot the web container name:

```bash
$ docker container ls --filter name=web1
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES
62782dc5f612        nginx:alpine        "nginx -g 'daemon ..."   30 seconds ago      Up 29 seconds       0.0.0.0:8000->80/tcp   web1
```

The top line shows the new **web1** container running NGINX. Take note of the command the container is running as well as the port mapping - `0.0.0.0:8000->80/tcp` maps port 8000 on all host interfaces to port 80 inside the **web1** container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8000).

Now that the container is running and mapped to a port on a host interface you can test connectivity to the NGINX web server, by browsing to the master node.

IN the PWD session information, you will see the **UCP Hostname** listed - it will be a long domain, something like `ip172-18-0-6-bc2m2d8qitt0008vqor0.direct.beta-hybrid.play-with-docker.com`. Browse to port `8000` at that address and you will see Nginx running - Docker receives the request and routes it into the container.

> **NOTE:** The port mapping is actually port address translation (PAT).

## <a name="task3"></a>Section #3 - Overlay Networking

Overlay networks span all the nodes in a Docker swarm mode cluster. Docker EE is built on top of swarm mode. You have multiple nodes in your lab environment so you can verify that containers in an overlay network can reach it other, even if they are running on different hosts.

### <a name="connect-container"></a>Step 1: The Basics

Run `docker node ls` on **manager1** to verify that you have multiple active nodes in the swarm:

```bash
$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
lo6fasr4izuz6okhm411vmyu3 *   manager1            Ready               Active              Leader
o0b82a6jwp3jfibixttb06fk8     worker3             Ready               Active
u4xc7yyu1j8hrea4dudr51ncx     worker2             Ready               Active
wuy5h23ly2y4wx5ccrjiscohv     worker1             Ready               Active
```

The `ID` and `HOSTNAME` values may be different in your lab. The important thing to check is that both nodes have joined the Swarm and are *ready* and *active*.

## <a name="create_network"></a>Step 2: Create an overlay network

Now that you have a Swarm initialized it's time to create an **overlay** network.

Create a new overlay network called **overnet** by running `docker network create -d overlay overnet` on **manager1**.

```bash
$ docker network create -d overlay overnet
wlqnvajmmzskn84bqbdi1ytuy
```

Use the `docker network ls` command to verify the network was created successfully:

```bash
$ docker network ls --filter name=overnet
NETWORK ID          NAME                DRIVER              SCOPE
2s4a0n00wdhd        overnet             overlay             swarm
```

The new "overnet" network is associated with the **overlay** driver and is scoped to the entire swarm - compare this to the **bridge** and **nat** networks which are locally scoped to a single node.

Run the `docker network ls` command on **worker1** to show all the overlay networks:

```bash
$ docker network ls --filter driver=overlay
NETWORK ID          NAME                DRIVER              SCOPE
55f10b3fb8ed        bridge              bridge              local
b7b30433a639        docker_gwbridge     bridge              local
a7449465c379        host                host                local
8hq1n8nak54x        ingress             overlay             swarm
06c349b9cc77        none                null                local
```

> The **overnet** network is not in the list. This is because Docker only extends overlay networks to hosts when they are needed. This is usually when a host runs a task from a service that is created on the network. We will see this shortly.

Use the `docker network inspect` command to view more detailed information about the "overnet" network. You will need to run this command from **manager1**.

```bash
$ docker network inspect overnet
[
    {
        "Name": "overnet",
        "Id": "wlqnvajmmzskn84bqbdi1ytuy",
        "Created": "0001-01-01T00:00:00Z",
        "Scope": "swarm",
        "Driver": "overlay",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": []
        },
        "Internal": false,
        "Attachable": false,
        "Containers": null,
        "Options": {
            "com.docker.network.driver.overlay.vxlanid_list": "4097"
        },
        "Labels": null
    }
]
```

### <a name="create_service"></a>Step 3: 创建服务

Now that we have an overlay network in our Docker EE cluster, it's time to create a service that uses the network.

Execute the following command from **manager1** to create a new service called *ubuntu* on the *overnet* network with two tasks/replicas.

```bash
docker service create --name ubuntu --detach \
--network overnet \
--replicas 6 \
sixeyed/ubuntu-with-utils sleep infinity
```

Verify that the service is created and both replicas are up by running `docker service ls` and filtering on the service name:

```bash
$ docker service ls --filter name=ubuntu
ID                  NAME                MODE                REPLICAS            IMAGE                              PORTS
yxt9w3573qb5        ubuntu              replicated          6/6                 sixeyed/ubuntu-with-utils:latest                ubuntu:latest
```

The `6/6` in the `REPLICAS` column shows that all the tasks in the service are up and running.

Verify that the tasks (replicas) are running on different swarm nodes `docker service ps`:

```bash
$ docker service ps ubuntu
ID                  NAME                IMAGE                              NODE                DESIRED STATE       CURRENT STATE                ERROR               PORTS
5z0u26v62qof        ubuntu.1            sixeyed/ubuntu-with-utils:latest   worker2             Running             Running about a minute ago
ph7ps3273w5m        ubuntu.2            sixeyed/ubuntu-with-utils:latest   manager1            Running             Running about a minute ago
tueyl7niinsb        ubuntu.3            sixeyed/ubuntu-with-utils:latest   worker3             Running             Running about a minute ago
...
```

The `ID` and `NODE` values will be different in your output. The important thing to note is that every Linux node is running at least one replica, which means multiple containers across the swarm, all connected to the same overlay network.

Now that all the nodes are running tasks on the "overnet" network, you will be able to see that network from every node. Connect to **worker2** and verify that by running `docker network ls` and filtering for overlay networks:

```bash
$ docker network ls --filter driver=overlay
NETWORK ID          NAME                DRIVER              SCOPE
jfy2cbcbw567        ingress             overlay             swarm
j20kcrvwhqw4        overnet             overlay             swarm
```

> You'll learn about the **ingress** network later in this workshop.

We can also run `docker network inspect overnet` on **worker2** to get more detailed information about the "overnet" network and obtain the IP address of the task:

```bash
$ docker network inspect overnet
[
    {
        "Name": "overnet",
        "Id": "j20kcrvwhqw45ujxay2m7q1af",
        "Created": "2018-05-23T15:19:36.669608649Z",
        "Scope": "swarm",
        "Driver": "overlay",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "10.0.0.0/24",
                    "Gateway": "10.0.0.1"
                }
            ]
        },
        ...
        "Containers": {
            "67264d255b873567c30375e8d021146cbae7c0a162c9297d143471fd14e0ce54": {
                "Name": "ubuntu.5.4rrqbneiay7uau63m07blofh5",
                "EndpointID": "a1530106c077896f4c24ae7054039a9ea728ae72831955802e92ac8c3bf31075",
                "MacAddress": "02:42:0a:00:00:0b",
                "IPv4Address": "10.0.0.11/24",
                "IPv6Address": ""
            }
            ...
```

> The subnet is specific to the **overnet** network in the swarm. In this case it is **10.0.0.0/24**, which means any containers attached to the network will have an IP address in the range **10.0.0.1** to **10.0.0.254**.

You should note that `docker network inspect` only shows containers/tasks running on the local node. This means that **10.0.0.11** is the IPv4 address of the container running on **worker2**. Make a note of this IP address for the next step (the IP address in your lab might be different than the one shown here in the lab guide).

### <a name="test"></a>Step 4: Test the network

To complete this step you will need the IP address of the service task running on the worker that you saw in the previous step (in this example it was **10.0.0.11**).

Execute the following commands from **worker3**, to verify that containers on this node can connect to containers running on **worker2**.

Store the ID of the service task container to a varable:

```bash
id=$(docker container ls --last 1 --format "{{ .ID }}")
```

And now confirm you can ping the container running on **worker3** from the container running on **worker2**:

```bash
$ docker container exec $id ping -c 2 10.0.0.11
PING 10.0.0.11 (10.0.0.11) 56(84) bytes of data.
64 bytes from 10.0.0.11: icmp_seq=1 ttl=64 time=0.163 ms
64 bytes from 10.0.0.11: icmp_seq=2 ttl=64 time=0.174 ms
```

The output above shows that all the tasks from the **ubuntu** service are on the same overlay network spanning multiple nodes and that they can use this network to communicate.

### <a name="discover"></a>Step 5: Test service discovery

Now that you have a working service using an overlay network, let's test service discovery.

Still on **worker2**, use the container ID you have stored to see how DNS resolution is configured in containers. Run `cat /etc/resolv.conf`:

```bash
$ docker container exec $id cat /etc/resolv.conf
search i4it0iff0fxurmesl4lntkyo2a.bx.internal.cloudapp.net
nameserver 127.0.0.11
options ndots:0 ndots:0
```

The value that we are interested in is the `nameserver 127.0.0.11`. This value sends all DNS queries from the container to an embedded DNS resolver running inside the container listening on 127.0.0.11:53. All Docker container run an embedded DNS server at this address.

> **NOTE:** Some of the other values in your file may be different to those shown in this guide.

Try and ping the "ubuntu" name from within the container by running `ping -c2 ubuntu`.

```bash
$ docker container exec $id ping -c 2 ubuntu
PING ubuntu (10.0.0.2) 56(84) bytes of data.
64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.038 ms
64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.041 ms
...
```

The output clearly shows that the container can ping the `ubuntu` service by name. Notice that the IP address returned is `10.0.0.2`. In the next few steps we'll verify that this address is the virtual IP (VIP) assigned to the `ubuntu` service.

Inspect the configuration of the service by running `docker service inspect ubuntu` on the manager node **manager1**. Lets verify that the VIP value matches the value returned by the previous `ping` command.

```bash
$ docker service inspect ubuntu
[
    {
        "ID": "sua78ut856kq2h1q0dq1lbxik",
        "Version": {
            "Index": 242
        },
        "CreatedAt": "2018-06-01T10:43:43.491494388Z",
        "UpdatedAt": "2018-06-01T10:43:43.493785595Z",
        "Spec": {
            "Name": "ubuntu",
            "Labels": {},
            "TaskTemplate": {
                "ContainerSpec": {
                    "Image": "sixeyed/ubuntu-with-utils:latest@sha256:d6d109a6bc6b610992a9923b6089400e5150bbdc10539d1c1a8f67381bd7f738",
                    "Args": [
                        "sleep",
                        "infinity"
                    ],
                    "StopGracePeriod": 10000000000,
                    "DNSConfig": {}
                },
...
            "EndpointSpec": {
                "Mode": "vip"
            }
        },
        "Endpoint": {
            "Spec": {
                "Mode": "vip"
            },
            "VirtualIPs": [
                {
                    "NetworkID": "ke2flaxd3nby3zm0bs0fc4dng",
                    "Addr": "10.0.0.2/24"
                }
            ]
        }
    }
]
```

Towards the bottom of the output you will see the VIP of the service listed. The VIP in the output above is `10.0.0.2` but the value may be different in your setup. The important point to note is that the VIP listed here matches the value returned by the `ping` command.

Now that you're connected to **manager1** you can repeate the same `ping` command using the container running on the manager - you will get a response form the same VIP:

```bash
$ id=$(docker container ls --last 1 --format "{{ .ID }}")

$ docker container exec $id ping -c 2 ubuntu
PING ubuntu (10.0.0.2) 56(84) bytes of data.
64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.046 ms
64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.043 ms
...
```

### <a name="routingmesh"></a>Step 6: 测试 Routing Mesh

Now let's create a service that utilizes Routing Mesh and the ingress network. Here you'll be creating a single task service that exposes port 5000 on the ingress network.

Create the service with a single replica, using the **manager1** node:

```
docker service create -p 5000:5000 -d --name pets --replicas=1 nicolaka/pets_web:1.0
```

Browse to UCP and using the left navigation click on _Swarm...Services_. You'll see the **pets** service in the list - click on the service and in the details panel on the right you can see a link to the published endpoint:

[](img/swarm/pets-service-link.jpg)

Click the link and the app will open in a new browser tab:

[](img/swarm/pets-1.jpg)

> The domain name you're browsing to is the UCP manager node. The ingress network receives the request and routes it to one of the service tasks - any node in the cluster can respond to the request by internally routing it to a container.

Try scaling up the service. In UCP select the **pets** service and click _Configure_:

[](img/swarm/pets-configure.jpg)

Select the _Scheduling_ section, and run more tasks by setting the _Scale_ level to 10:

[](img/swarm/pets-scale.jpg)

Click save and UCP returns to the service list. The service is scaling up and you can see the container list by clicking on _Inspect Resource...Containers_:

[](img/swarm/pets-container-list.jpg)

You'll see containers running on nodes across the cluster. Now refresh the tab with the Pets website. Each time you refresh you'll see a different container ID. Docker swarm load-balances requests across all the tasks in the service.

### 清除环境

删除创建的 swarm mode 服务:

```bash
docker service rm pets
docker service rm ubuntu
docker network rm overnet
```

### 继续

Next we'll move onto [networking in Kubernetes](kube.md). You'll learn how to use Kubernetes in Docker EE to enforce network policies, and restrict which containers can access other containers.

参考文章

## [How Docker Swarm Container Networking Works](https://neuvector.com/network-security/docker-swarm-container-networking/)

### 部署

首先, 创建一个 overlay network 来部署我们的容器. 我们的Swarm 环境有两个节点，在这里我们将创建三个服务，每个服务运行一个实例.

``` bash
docker network create --opt encrypted --subnet 100.0.0.0/24 -d overlay net1

docker service create --name redis --network net1 redis
docker service create --name node --network net1 nvbeta/node
docker service create --name nginx --network net1 -p 1080:80 nvbeta/swarm_nginx

```

以上命令创建了一个典型的3层应用，'nginx' 容器作为LB负载均衡器将请求转发到后端的web服务器'node',最后node容器访问'redis’数据库容器，然后返回结果给用户，简单起见，我们的nginx负载只连接一个web 服务。
应用的逻辑架构如图

![逻辑架构图](https://davidsche.github.io/blogs/images/logicview.png)

### 网络

让我们看一下 Docker Swarm 创建的网络,

```bash
$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
cac91f9c60ff        bridge              bridge              local
b55339bbfab9        docker_gwbridge     bridge              local
fe6ef5d2e8e8        host                host                local
f1nvcluv1xnf        ingress             overlay             swarm
8vty8k3pejm5        net1                overlay             swarm
893a1bbe3118        none                null                local
```

net1:

> 这个 overlay 网络是由我们创建，用于应用的各容器之间通讯.

docker_gwbridge:

> 这个网络由Docker创建. 它允许容器连接到所运行的宿主机.

ingress:

> 这个网络由Docker创建. Docker swarm 使用这个网络对外部网络公开服务并提供 routing mesh能力.

net1

因为所有的服务时都使用了 “–network net1” 选项, 每个服务都必须至少有一个接口连接到 ‘net1’ 网络.

我们以 node 1 节点为例. Docker Swarm在 node 1 节点部署了两个容器/服务.

``` bash
$ docker ps
CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS              PORTS               NAMES
eb03383913fb        nvbeta/node:latest   "nodemon /src/index.j"   2 hours ago         Up 2 hours          8888/tcp            node.1.10yscmxtoymkvs3bdd4z678w4
434ce2679482        redis:latest         "docker-entrypoint.sh"   2 hours ago         Up 2 hours          6379/tcp            redis.1.1a2l4qmvg887xjpfklp4d6n7y
```

通过为docker netns 目录常见一个内链接，我们可以发现node 1上的所有网络命名空间.

``` bash
$ cd /var/run
$ sudo ln -s /var/run/docker/netns netns
$ sudo ip netns
be663feced43
6e9de12ede80
2-8vty8k3pej
1-f1nvcluv1x
72df0265d4af

```

比较命名空间的名字和Docker swarm 网络 IDs,我们猜测ID 是‘8vty8k3pejm5’的‘net1’网络使用的是命名空间‘2-8vty8k3pej’,这可由比较命名空间的接口和容器来得到.

**列出容器中的接口:**

``` bash
$ docker exec node.1.10yscmxtoymkvs3bdd4z678w4 ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
11040: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:65:00:00:03 brd ff:ff:ff:ff:ff:ff
11042: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:ac:12:00:04 brd ff:ff:ff:ff:ff:ff

```

```bash
$ docker exec redis.1.1a2l4qmvg887xjpfklp4d6n7y ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
11036: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:65:00:00:08 brd ff:ff:ff:ff:ff:ff
11038: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff
```

**命名空间中的接口:**

``` bash
$ sudo ip netns exec 2-8vty8k3pej ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default
    link/ether 22:37:32:66:b0:48 brd ff:ff:ff:ff:ff:ff
11035: vxlan1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master br0 state UNKNOWN mode DEFAULT group default
    link/ether 2a:30:95:63:af:75 brd ff:ff:ff:ff:ff:ff
11037: veth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master br0 state UP mode DEFAULT group default
    link/ether da:84:44:5c:91:ce brd ff:ff:ff:ff:ff:ff
11041: veth3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master br0 state UP mode DEFAULT group default
    link/ether 8a:f9:bf:c1:ec:09 brd ff:ff:ff:ff:ff:ff
```

**请注意**，' br0 '是连接所有接口的Linux Bridge; ' vxlan1 '是VXLAN overlay 网络的VTEP接口。对于Docker为容器创建的每个veth对(veth pair)，容器内的设备始终具有ID号，该ID号比另一端的设备ID小1。因此，命名空间中的veth2（11037）连接到redis容器中的eth0（11036）; 命名空间中的veth3（11041）连接到节点容器中的eth0（11040）.

我们现在可以确定命名空间' 2-8vty8k3pej '用于'net1'覆盖网络。我们可以根据我们学到的信息在节点1上绘制网络图.

``` bash

                node 1

  +-----------+      +-----------+
  |  nodejs   |      |   redis   |
  |           |      |           |
  +--------+--+      +--------+--+
           |                  |
           |                  |
           |                  |
           |                  |
      +----+------------------+-------+ net1
       101.0.0.3          101.0.0.8
       101.0.0.4(vip)     101.0.0.2(vip)

```

### docker_gwbridge

我们可以通过列出主机上的接口比较 ‘redis’ and ‘node’ 容器接口.

列出主机上的接口：

``` bash
$ ip link
...
4: docker_gwbridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:24:f1:af:e8 brd ff:ff:ff:ff:ff:ff
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
    link/ether 02:42:e4:56:7e:9a brd ff:ff:ff:ff:ff:ff
11039: veth97d586b: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default
    link/ether 02:6b:d4:fc:8a:8a brd ff:ff:ff:ff:ff:ff
11043: vethefdaa0d: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default
    link/ether 0a:d5:ac:22:e7:5c brd ff:ff:ff:ff:ff:ff
10876: vethceaaebe: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default
    link/ether 3a:77:3d:cc:1b:45 brd ff:ff:ff:ff:ff:ff
...
```

回想一下veth对应的设备ID，我们知道，宿主机上的 veth97d586b（11039）连接到redis容器中的eth1（11038）; 宿主机上的vethefdaa0d（11043）连接到节点容器中的eth1（11042）。

宿主机上的这两个接口位于docker_gwbridge桥上，

``` bash
$ brctl show
bridge name            bridge id                      STP enabled            interfaces
docker0                8000.0242e4567e9a              no
docker_gwbridge        8000.024224f1afe8              no                     veth97d586b
                                                                             vethceaaebe
                                                                             vethefdaa0d
```

每台主机上的docker_gwbridge非常类似于单主机Docker环境中的docker0网桥。每个容器都有一条连接到它的链路，它可以从运行容器的主机到达。

可以更新节点1的网络图，并添加主机网络。

```bash
             node 1

  172.18.0.4         172.18.0.3
 +----+------------------+----------------+ docker_gwbridge
      |                  |
      |                  |
      |                  |
      |                  |
   +--+--------+      +--+--------+
   |  nodejs   |      |   redis   |
   |           |      |           |
   +--------+--+      +--------+--+
            |                  |
            |                  |
            |                  |
            |                  |
       +----+------------------+----------+ net1
        101.0.0.3          101.0.0.8
        101.0.0.4(vip)     101.0.0.2(vip)
```

然而, 与 docker0 bridge不同, 他不用于连接外部网络. 对于公开端口的Docker Swarm 服务(使用 -p 参数), Docker 为它创建了一个 dedicated ‘ingress’ 网络.

### **ingress**

我们再次列出node 1节点上的Docker Swarm  网络和网络命名空间

``` bash
$ sudo ip netns
be663feced43
6e9de12ede80
2-8vty8k3pej
1-f1nvcluv1x
72df0265d4af

$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
cac91f9c60ff        bridge              bridge              local
b55339bbfab9        docker_gwbridge     bridge              local
fe6ef5d2e8e8        host                host                local
f1nvcluv1xnf        ingress             overlay             swarm
8vty8k3pejm5        net1                overlay             swarm
893a1bbe3118        none                null                local

```

明显, 1-f1nvcluv1x 是 ‘ingress’ 网络的命名空间, 但是 72df0265d4af 的命名空间是为谁用的呢?

让我们来看一下这个命名空间中有些什么接口

``` bash
$ sudo ip netns exec 72df0265d4af ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
10873: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default
    link/ether 02:42:0a:ff:00:03 brd ff:ff:ff:ff:ff:ff
    inet 10.255.0.3/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:aff:feff:3/64 scope link
       valid_lft forever preferred_lft forever
10875: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.2/16 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe12:2/64 scope link
       valid_lft forever preferred_lft forever

```

eth1（10875）与宿主上的vethceaaebe（10876）配对; 我们还可以发现eth0连接到'ingress' overlay 网络。接口设置看起来像容器的网络命名空间。

这可以通过检查(inspect)节点node 1上的Docker Swarm'ingress'和'docker_gwbridge'网络来证明。

``` json
$ docker network inspect ingress
[
    {
        "Name": "ingress",
        "Id": "f1nvcluv1xnfa0t2lca52w69w",
        "Scope": "swarm",
        "Driver": "overlay",
        ....
        "Containers": {
            "ingress-sbox": {
                "Name": "ingress-endpoint",
                "EndpointID": "3d48dc8b3e960a595e52b256e565a3e71ea035bb5e77ae4d4d1c56cab50ee112",
                "MacAddress": "02:42:0a:ff:00:03",
                "IPv4Address": "10.255.0.3/16",
                "IPv6Address": ""
            }
        },
        ....
    }
]

$ docker network inspect docker_gwbridge
[
    {
        "Name": "docker_gwbridge",
        "Id": "b55339bbfab9bdad4ae51f116b028ad7188534cb05936bab973dceae8b78047d",
        "Scope": "local",
        "Driver": "bridge",
        ....
        "Containers": {
            ....
            "ingress-sbox": {
                "Name": "gateway_ingress-sbox",
                "EndpointID": "0b961253ec65349977daa3f84f079ec5e386fa0ae2e6dd80176513e7d4a8b2c3",
                "MacAddress": "02:42:ac:12:00:02",
                "IPv4Address": "172.18.0.2/16",
                "IPv6Address": ""
            }
        },
        ....
    }
]
```

这些网络上的端点MAC/IP 地址与命名空间72df0265d4af中的接口MAC/IP 地址匹配。此命名空间用于名为“ ingress-sbox”的隐藏容器，该容器在主机网络上有一条链路，在“入口”网络上有另一条链路。

Docker Swarm的一个主要功能特性是发布/公开端口的容器的 ‘routing mesh’,无论容器实例实际运行在哪个节点上，您都可以通过任何节点访问它。怎么做？让我们深入挖掘这个隐藏的容器。

在我们的应用程序中，它是'nginx'服务，它发布端口80并映射到主机上的端口1080，但'nginx'容器未在节点1上运行。

仍然是 node 1 节点,

``` bash
$ sudo iptables -t nat -nvL
...
...
Chain DOCKER-INGRESS (2 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:1080 to:172.18.0.2:1080
 176K   11M RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0

$ sudo ip netns exec 72df0265d4af iptables -nvL -t nat
...
...
Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    9   576 REDIRECT   tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:1080 redir ports 80
...
...
Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DOCKER_POSTROUTING  all  --  *      *       0.0.0.0/0            127.0.0.11
   14   896 SNAT       all  --  *      *       0.0.0.0/0            10.255.0.0/16        ipvs to:10.255.0.3

```

如您所见，iptables规则将到端口1080流量重定向到隐藏容器“ingress-sbox”中的端口80。然后POSTROUTING 链将数据包放在IP 10.255.0.3上，这是 ‘ingress’ 网络上接口的IP地址。

**注意**：SNAT规则中的'ipvs'.' ipvs '是Linux内核中的负载均衡器实现。这是一个鲜为人知的工具，已经在内核中存在了16年。

``` bash
$ sudo ip netns exec 72df0265d4af iptables -nvL -t mangle
Chain PREROUTING (policy ACCEPT 144 packets, 12119 bytes)
 pkts bytes target     prot opt in     out     source               destination
   87  5874 MARK       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:1080 MARK set 0x12c
...
...
Chain OUTPUT (policy ACCEPT 15 packets, 936 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 MARK       all  --  *      *       0.0.0.0/0            10.255.0.2           MARK set 0x12c
...
...

```

iptables 规则将流(flow)标记为0x12c（= 300），下面是ipvs的配置信息，

``` bash
$ sudo ip netns exec 72df0265d4af ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
FWM  300 rr
  -> 10.255.0.5:0                 Masq    1      0          0
```

10.255.0.5是另一个节点上'nginx'容器的IP.它是负载均衡器的唯一后台服务器。

随着一切的汇总起来，我们可以更新我们的网络连接视图（单击以展开）

![img](https://davidsche.github.io/blogs/images/swarm_network_view.png)

**总结:**

使用Docker Swarm容器网络时，会发生很多很酷的事情。这使得在多主机网络上部署更复杂的生产应用程序变得容易。使用overlay网络可以轻松地跨节点甚至跨云环境扩展和移动容器。但是，了解底层发生的情况总是有用的，因此可以更轻松地调试任何连接问题。容器之间“ 东-西向 ”交通的增加也意味着需要新的可见性和安全性方法。

[For an overview of Kubernetes networking and security, download The Ultimate Kubernetes Security Guide](https://neuvector.com/network-security/docker-swarm-container-networking/#fancyboxID-1)



