# Docker 最佳实践系列  HA Spark服务

-----

## 在docker swarm环境中部署 HA Spark服务

H/A Spark Cluster on Docker Swarm

[英文原文:](https://github.com/BirgerK/docker-apache-spark/blob/master/Dockerfile)

### 1. 准备工作

获取部署文件

``` Dockerfile

FROM phusion/baseimage:0.9.22
MAINTAINER BirgerK <birger.kamp@gmail.com>

ENV SPARK_VERSION 2.2.1
ENV SPARK_INSTALL /usr/local
ENV SPARK_HOME $SPARK_INSTALL/spark
ENV SPARK_ROLE master
ENV HADOOP_VERSION 2.7
ENV SPARK_MASTER_PORT 7077
ENV PYSPARK_PYTHON python3
ENV DOCKERIZE_VERSION v0.2.0

RUN apt-get update && \
    apt-get install -y openjdk-8-jdk autossh python3-pip && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

##### INSTALL DOCKERIZE
RUN curl -L -O https://github.com/jwilder/dockerize/releases/download/$DOCKERIZE_VERSION/dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz && \
    tar -C /usr/local/bin -xzvf dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz && \
    rm -rf dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz

##### INSTALL APACHE SPARK WITH HDFS
RUN curl -s http://mirror.synyx.de/apache/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz | tar -xz -C $SPARK_INSTALL && \
    cd $SPARK_INSTALL && ln -s spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION spark

WORKDIR $SPARK_HOME

##### ADD Scripts
RUN mkdir /etc/service/spark
ADD runit/spark.sh /etc/service/spark/run
RUN chmod +x /etc/service/**/*

EXPOSE 4040 6066 7077 7078 8080 8081 8888

VOLUME ["$SPARK_HOME/logs"]

CMD ["/sbin/my_init"]
```

/runit/spark.sh

``` bash
#!/bin/bash

if [ "$SPARK_ROLE" = "master" ]; then
  $SPARK_HOME/bin/spark-class org.apache.spark.deploy.master.Master
fi
if [ "$SPARK_ROLE" = "slave" ]; then
  $SPARK_HOME/bin/spark-class org.apache.spark.deploy.worker.Worker spark://$SPARK_MASTER:$SPARK_MASTER_PORT
fi
```

部署docker-compose.yml

```  yml

sparkmaster:
  image: birgerk/apache-spark
  ports:
    - 4040:4040
    - 7077:7077
    - 8080:8080
  environment:
    SPARK_ROLE: master

sparkslave1:
  image: birgerk/apache-spark
  entrypoint: dockerize -wait tcp://sparkmaster:7077 -timeout 240s /sbin/my_init
  ports:
    - 8081:8081
  links:
    - sparkmaster
  environment:
    SPARK_MASTER: sparkmaster
    SPARK_ROLE: slave

sparkslave2:
  image: birgerk/apache-spark
  entrypoint: dockerize -wait tcp://sparkmaster:7077 -timeout 240s /sbin/my_init
  ports:
    - 8091:8081
  links:
    - sparkmaster
  environment:
    SPARK_MASTER: sparkmaster
    SPARK_ROLE: slave
```

[参考](https://gridscale.io/en/community/tutorials/how-to-run-services-on-docker-swarm/)

